# Nosana Dashboard Scraper Configuration Guide

## Overview
This guide explains how to configure the Nosana dashboard scraper to identify clickable elements and avoid unwanted interactions.

## What Information You Need to Provide

### 1. Clickable Element Identification

The scraper uses CSS selectors to identify clickable elements. You can customize these by providing:

#### **Default Clickable Selectors** (already configured):
```python
clickable_selectors = [
    'a[href]',                    # Links with href attribute
    'button[onclick]',            # Buttons with onclick handlers
    '[role="button"]',            # Elements with button role
    '.clickable',                 # Elements with 'clickable' class
    '[data-testid*="link"]',      # Test ID containing 'link'
    '[data-link]',                # Elements with data-link attribute
]
```

#### **How to Add Custom Selectors**:
If the site uses specific patterns, you can add:
```python
config.clickable_selectors.extend([
    '.custom-button-class',       # Specific CSS classes
    '[data-action]',              # Data attributes
    '.menu-item a',               # Nested selectors
    'div[onclick]',               # Specific element types
])
```

### 2. Element Exclusion Strategies

#### **By Text Content** (what you provided):
```python
excluded_elements = [
    'Profile',
    'Deploy Model', 
    'Explorer',
    'Help & Support',
    'Healthy',
    'Nosana dashboard',
    'Select Wallet',
    'Â© 2025 Nosana'
]
```

#### **By Text Patterns** (safety patterns):
```python
excluded_text_patterns = [
    'logout',
    'sign out',
    'delete',
    'remove',
    'cancel',
    'close account'
]
```

#### **By CSS Classes/Attributes**:
The scraper automatically excludes elements with these classes:
- `nav`, `menu`, `header`, `footer`, `sidebar`
- Elements with `data-exclude="true"`

### 3. How to Determine What's Clickable

#### **For JavaScript-Heavy Sites**:
If the site uses JavaScript/React (like Nosana dashboard):

1. **Inspect Elements**: Use browser dev tools (F12) to see:
   ```html
   <button onclick="handleClick()">Click me</button>
   <div role="button" data-testid="action-button">Action</div>
   <a href="/page">Link</a>
   ```

2. **Look for Patterns**:
   - `role="button"`
   - `data-testid` attributes
   - `onclick` handlers
   - `cursor: pointer` in CSS

3. **Check for Dynamic Elements**:
   ```javascript
   // Elements that become clickable after page load
   document.querySelector('.dynamic-button').addEventListener('click', ...)
   ```

#### **For Traditional HTML Sites**:
- `<a href="...">` tags
- `<button>` elements
- `<input type="submit">` or `<input type="button">`

### 4. Configuration Examples

#### **Basic Configuration**:
```python
config = ScrapingConfig(
    base_url="https://dashboard.nosana.com/",
    max_depth=1,
    delay_between_requests=2.0,
    output_format="json"
)
```

#### **Advanced Configuration**:
```python
config = ScrapingConfig(
    base_url="https://dashboard.nosana.com/",
    max_depth=2,  # Go 2 clicks deep
    delay_between_requests=3.0,  # Slower for rate limiting
    timeout=15,  # Longer timeout for slow pages
    output_format="json"
)

# Add custom clickable elements
config.clickable_selectors.extend([
    '.dashboard-card a',          # Dashboard card links
    '[data-navigate]',            # Custom navigation attributes
    '.btn-primary',               # Primary buttons
])

# Add more exclusions
config.excluded_elements.extend([
    'Settings',
    'Logout',
    'Admin Panel',
    'Support Chat'
])

# Add pattern exclusions
config.excluded_text_patterns.extend([
    'beta',
    'coming soon',
    'under maintenance'
])
```

### 5. Depth Configuration

#### **Depth Levels**:
- **Depth 0**: Only the main page
- **Depth 1**: Main page + one click deep
- **Depth 2**: Main page + one click + second level clicks
- **Depth 3+**: Increasingly deep traversal

#### **Adjusting Depth**:
```python
# Conservative (fast)
config.max_depth = 1

# Moderate (balanced)
config.max_depth = 2

# Aggressive (comprehensive but slow)
config.max_depth = 3
```

### 6. Site Analysis Tools

#### **Before Scraping, Analyze the Site**:

1. **Use Browser Dev Tools**:
   ```javascript
   // In browser console, find all clickable elements
   document.querySelectorAll('a, button, [onclick], [role="button"]')
   ```

2. **Check Network Tab**: See what requests are made when clicking elements

3. **Look for Patterns**: 
   - Are URLs in `href` attributes?
   - Are they generated by JavaScript?
   - Do they use hash routing (`#/page`)?

### 7. Common Site Types & Strategies

#### **React/SPA Sites** (like Nosana):
- Look for `data-testid` attributes
- Check for `role="button"` 
- URLs might be hash-based (`#/dashboard`)
- May need to wait for dynamic content

#### **Traditional Sites**:
- Standard `<a href>` links
- Form submissions
- Simple button clicks

#### **API-Driven Dashboards**:
- Elements may trigger API calls
- Data loaded dynamically
- May need authentication

### 8. Running the Scraper

#### **Install Dependencies**:
```bash
pip install -r requirements.txt
```

#### **Basic Usage**:
```python
python nosana_scraper.py
```

#### **Custom Configuration**:
```python
from nosana_scraper import ScrapingConfig, NosanaScraper

# Your custom config
config = ScrapingConfig(
    base_url="https://dashboard.nosana.com/",
    max_depth=2,
    excluded_elements=['Profile', 'Logout', 'Settings']
)

scraper = NosanaScraper(config)
results = scraper.scrape_with_depth()
scraper.save_results("my_scrape_results")
```

### 9. Output Formats

#### **JSON** (default):
```json
{
  "url": "https://dashboard.nosana.com/",
  "title": "Nosana Dashboard",
  "text_content": "...",
  "links": ["...", "..."],
  "depth": 0,
  "timestamp": "2024-01-01T12:00:00"
}
```

#### **CSV**:
Tabular format with columns for URL, title, content, etc.

#### **TXT**:
Human-readable text format with clear separations.

### 10. Troubleshooting

#### **Common Issues**:

1. **No Clickable Elements Found**:
   - Check if selectors match the site's HTML
   - Look at browser dev tools for actual element structure
   - Add custom selectors for the site

2. **Too Many Excluded Elements**:
   - Review exclusion lists
   - Make exclusion patterns more specific

3. **Authentication Required**:
   - May need to handle login flows
   - Consider cookie/session management

4. **Rate Limiting**:
   - Increase `delay_between_requests`
   - Implement exponential backoff

#### **Debug Mode**:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

This will show detailed information about what elements are being found and why they're included/excluded.

## Summary

To effectively scrape the Nosana dashboard:

1. **Provide text patterns** of elements you DON'T want clicked
2. **Specify CSS selectors** for clickable elements if defaults don't work
3. **Set appropriate depth** (start with 1, increase as needed)
4. **Configure delays** to respect the site's rate limits
5. **Test with low depth first** to understand the site structure

The scraper is designed to be safe by default - it will avoid common navigation elements and dangerous actions unless explicitly configured otherwise.